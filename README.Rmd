---
title: "ART-TREES Audit Repository Demo"
author: "Winrock Intl"
date: "`r Sys.Date()`"
output:
  github_document:
    toc: true
    toc_depth: 4
#    keep_md: true

always_allow_html: TRUE
editor_options: 
  markdown: 
    wrap: 100
---


```{r, echo=F, eval=T, message=F, warning=F, error=F, comment=NA}
## INTERNAL COPY - TRIM PRIOR SEMINAR

install.packages("easypackages")
easypackages::packages(
  "bslib",
  "caret", "cli", "cols4all", "covr", "cowplot",
  "e1071", "exactextractr", "elevatr",
  "gdalcubes", "gdalUtilities", "geojsonsf", "geos", "ggplot2", "ggstats",
  "ggspatial", "ggmap", "ggplotify", "ggpubr", "ggrepel", "giscoR",
  "hdf5r", "httr", "httr2", "htmltools",
  "jsonlite",
  "leafem", "libgeos", "luz", "lwgeom", "leaflet", "leafgl",
  "mapedit", "mapview", "maptiles", "methods", "mgcv",
  "ncdf4", "nnet",
  "openxlsx",
  "parallel", "plotly",
  "randomForest", "rasterVis", "raster", "Rcpp", "RcppArmadillo",
  "RcppCensSpatial", "rayshader", "RcppEigen", "RcppParallel", "renv",
  "RColorBrewer", "rgl", "RStoolbox", "rts",
  "s2", "sf", "scales", "spdep", "stars", "stringr",
  "terra", "testthat", "tidyverse", "tidyterra",
  "tmap", "tmaptools", "terrainr",
  "xgboost", prompt=FALSE)
  
knitr::opts_chunk$set(
  echo = TRUE, message = FALSE, warning = FALSE,
  error = FALSE, comment = NA, tidy.opts = list(width.cutoff = 6)
)
options(htmltools.dir.version = FALSE, htmltools.preserve.raw = FALSE)
mapviewOptions(fgb = FALSE)
sf::sf_use_s2(use_s2 = FALSE)
```

## Replication via Renv Snapshots

`renv` profiles allow for managing different sets of package dependencies for various project contexts, such as separate environments for development, production, or demonstrations.

## Activating a Renv Profile

To activate a specific `renv` profile, you have two primary options:

-  Step 1. Set a project profile: To make a profile the default for future R sessions, run `renv::activate()` function. Upon restarting R, you will find `lockfile` path assigned to new project `renv` folder ( `renv/profiles/demo-repo/`).
-  Step 2. Activate a temporary profile: To activate a profile for the current R session without making it the default, you can set the `RENV_PROFILE` environment variable.

## Managing Renv Dependencies

Dependencies for a specific profile can be declared within the project's top-level DESCRIPTION file. `renv` also allows you to specify custom package remotes separately from the dependencies.

-  Dependencies: Use the `Config/renv/profiles/<profile_name>/dependencies` field to list required packages.
-  Remotes: Use the` Config/renv/profiles/<profile_name>/remotes` field to define custom remotes for those packages.

You can also opt-in to `explicit` snapshots, which uses only the packages enumerated in this field and ignores the standard Imports, Depends, and Suggests fields.


```{r, eval=F}
# Activate 'demo-repo' profile and set as default for the project
renv::activate(profile = "demo-repo")

# To activate a 'dev' profile for a single session only
Sys.setenv(RENV_PROFILE = "dev")

# Example of defining profile-specific dependencies in DESCRIPTION
Config/renv/profiles/shiny/dependencies: shiny, tidyverse
Config/renv/profiles/shiny/remotes: rstudio/shiny, tidyverse/tidyverse

# Enable 'explicit' snapshots for the project
renv::settings$snapshot.type("explicit")
```

#options(repos = c(CRAN = "https://cloud.r-project.org"))
#readRenviron("~/.Renviron")
#readRenviron("~/.Rprofile")
#{bash}: "source  ~/.zshrc"
#chooseCRANmirror()

libs = c("devtools", 
         "ggmap", 
         "htmltools", 
         "janitor", "jsonlite", 
         "kableExtra", "knitr", 
         "latex2exp", 
         "leaflet", "leaflet.providers", "lwgeom", 
         "magrittr", "maptiles", "MASS", "methods", 
         "osmdata", 
         "pak", "pander", "psych", 
         "raster", "rasterVis", "RColorBrewer", "Rcpp", "readxl", "remotes",
         "rmarkdown", "rnaturalearth", "rnaturalearthdata", "RStoolbox", 
         "s2", "sf", "sp", "stars", "stringr", 
         "terra", "terrainr", "tibble", "tidyr", "tidyverse",  "tinytex", "tmap", "tmaptools", 
         "units")

installed_libs = libs %in% rownames(installed.packages())
if(any(installed_libs==F)){install.packages(libs[!installed_libs],repos="https://cloud.r-project.org")}
invisible(lapply(libs, library, character.only = T))
lapply(libs, require, character.only = T)

base::options(
  htmltools.dir.version = F, 
  htmltools.preserve.raw = F
  )

knitr::opts_chunk$set(
  echo    = TRUE, 
  message = FALSE, 
  warning = FALSE,
  error   = FALSE, 
  comment = NA,
  tidy.opts = list(width.cutoff = 120),
  fig.width  = 14,
  fig.height = 10
  ) 

# non-spherical geometries
sf::sf_use_s2(use_s2 = FALSE) 

# purge cache
terraOptions(memfrac=0.9, tempdir = "./temp")

```

# <img src="https://winrock.org/wp-content/uploads/2021/12/Winrock-logo-R.png" width="500"/>

[![LinkedIn
Badge](https://img.shields.io/badge/Project-Profile-blue)](https://www.linkedin.com/in/seamusrobertmurphy/)
[![Pubs
Badge](https://img.shields.io/badge/Project-Pubs-critical)](https://scholar.google.com/citations?hl=en&user=jDGq9I4AAAAJ)
[![Twitter Badge](https://img.shields.io/badge/Project-Tweets-critical?color=blue)](https://x.com/)
[![Program
Badge](https://img.shields.io/badge/Project-Stewards-critical)](https://www.ambiente.gob.ec/)
[![Annexes
Badge](https://img.shields.io/badge/Project-Annexes-critical?color=blue)](https://nextcloud.ambiente.gob.ec)

-   [Installation](#installation)

-   [Repository Layout](#repository-layout)

-   [Repository Protocols](#repository-practices)

    -   [1. Data Traceability](#1-data-traceability)
    -   [2. Data Completeness](#2-data-completeness)
    -   [3. Data Curation](#3-data-curation)
    -   [4. Community Health](#4-community-health)

-   [Practice Exercise: A Mock Audit](#practice-exercise)

Welcome to the
[`ART-TREES-Verification-Repository-Demo-2025.git`](https://github.com/seamusrobertmurphy/ART-TREES-Verification-Repository-Demo-2025),
a training resource for preparing and submitting a complete data package for an ART TREES
verification audit. You will learn the gold standard of data practice: using version control to
ensure perfect reproducibility of your calculations. Today, we hope to

1.  **Clone this repository** to your local compute

2.  **Run the R Markdown analysis script** with one click

3.  **Achieve exact replication** of forest cover change calculations from the TMR

4.  **Demonstrate audit readiness** through version-controlled data practices

This exercise will test your ability to use version-controlled data to achieve an exact result,
demonstrating a key skill required for audit readiness. Specifically, we will replicate calculations
of change in forest cover for the crediting period from 2017 to 2021 using the example script and
markdown runtime named `TREES-TMR-Replication-Demo.Rmd`. Your final calculated values must match the
figures reported in **Table 16** of the previous TMR submission.

### Prerequisites {#pre-requis}

Before starting, ensure you have the following environment dependencies installed. See Appendix I
for installation resources.

-   Git installed on your computer
-   R, RStudio, & Git-CLI installed
-   Basic familiarity with R Markdown
-   Internet connection for cloning

### Installation {#installation}

**Instructions:**

#### **Step 1. Clone the Repository**

Open your terminal command-line tool (or GitHub Desktop) and clone this repository to your local
computer. This step is crucial for working with the precise, version-controlled data.

```` bash
```{bash}
git clone https://github.com/YourOrganization/ART-TREES-Verification-Repository-Demo-2025.git
```
````

#### **Step 2. Navigate to the Script**

All the necessary data and the analysis script are located in the `/03_Spatial_Data/` directory,
following the layout provided in your training curriculum.

-   `03_Spatial_Data/TREES-TMR-Replication-Demo.Rmd`

#### **Step 3. Execute the Script**

Open the `/03_Spatial_Data/` in your R environment and run it. The script is designed to
automatically perform the raster-based calculation to determine the change in forest area and will
output the final result.

#### **Step 4. Compare Results**

The script's output will display a final calculated value (in hectares). Compare this result to the
figure reported in **Table 16** of the TMR. They should match exactly.

If the script's output does not match, this is a critical learning opportunity. It demonstrates how
even small discrepancies in data or code can lead to different results, which is a common audit
finding. It highlights the need for rigorous version control and a robust QA/QC process.

### Repository Layout {#repository-layout}

We offer the following example as a tentative layout. Please explore the folder structure and
examine the contents of each sample document, and provide feedback on improved structure more
appropriate to your project design. Repository Layout

-   01_Program_Data/

-   02_Carbon_Data/

-   03_Spatial_Data/

    -   `TREES-TMR-Replication-Demo.Rmd`

-   04_Uncertainty/

-   05_Safeguards/

-   06_QAQC_SOPs/

![Example repository layout derived for illustrative purposes in this training series of August
2025](01_Program_Data/Communications/assets/repo-layout-wide.png)

----------------------------------------------------------------------------------------------------

### Repository Protocol {#repository-protocol}

As you navigate this repository, keep the following key principles in mind. These are the
fundamental concepts that auditors use to evaluate your submission. We also encourage you to use
this repository as a checklist or reference for your own submissions.

#### 1. Data Traceability {#data-traceaility}

Auditors must be able to trust that your data is authentic, accurate, and has not been altered
without record. Our demo repository demonstrates this through:

-   **Version Control:** Files are named with clear, descriptive conventions to track changes. For
    example, you should have a final version of a map, not just a series of ambiguous draft files.
-   **QA/QC Procedures:** We’ve included a folder for `Quality Control` to show where you would
    document internal checks, ensuring that manual data entry and calculations are double-checked to
    catch errors.
-   **Traceability:** Every key figure in a report, such as `X hectares deforested`, must be
    traceable back to its raw data source and the methodology used to derive it. Our file structure
    helps link report claims to specific evidence files.

#### 2. Data Completeness {#data-completeness}

Completeness is a core principle of verification, meaning that all information needed to justify
your GHG assertion must be included. Nothing significant can be missing.

-   **Mapping Requirements:** The repository structure is organized to align directly with the ART
    TREES requirements. This provides a clear framework for you to check if you have all the
    necessary data for every required category, such as deforestation areas, emission factors, and
    safeguards information.
-   **Completeness Checklist:** You can create and use a checklist to confirm that data for all
    reporting periods, geographic areas, and carbon pools are present and documented before
    submission.
-   **Curation & Change-Logs:** The repository includes a place to document any methodological
    changes made over time. This ensures that the context for your data is complete, so auditors can
    fully understand your approach.

#### 3. Data Curation {#data-curation}

A well-organized data package allows auditors to quickly find what they need, making the
verification process more efficient and painless.

-   **Logical Folder Structure:** We recommend using a clear folder structure like the one in this
    demo.
-   Our layout, for example, separates `Calculations & Analysis` from `Spatial Data` and
    `SOPs & Procedures`. This logical grouping helps auditors instinctively navigate your
    submission.
-   **`README` Files:** A top-level `README` file acts as a quick guide to the entire repository.
    You can also add smaller `README` files to individual folders to describe their contents, like
    explaining which GIS files are included and what their purpose is.
-   **Descriptive Filenaming:** File names should be clear and descriptive, avoiding jargon. For
    example, `Deforestation_Map_2020.shp` is better than `map_v7_final.shp`.
-   **Spatial Data Checklist:** A critical part of any REDD+ submission is the spatial data.
    Auditors will check that all related files for a shapefile (e.g., `.shp`, `.shx`, `.dbf`) are
    included and that the coordinate reference system (`.prj` file) is present and correct. Our demo
    repository includes a section dedicated to this to ensure you don’t miss any components.

#### 4. Data Replication {#data-replication}

A well-organized .....

-   **Data Replication:** We recommend using .

#### 4. Community Health {#community-health}

We offer a shortlist of default community health files that may be adapted to include SOPs and
documentation of workbook updates, always aiming to build trust in the auditor:

-   **CODE_OF_CONDUCT.md:** A CODE_OF_CONDUCT file defines standards for how to engage in a
    community.
-   **CONTRIBUTING.md:** A CONTRIBUTING file communicates how people should contribute to your
    project. Discussion category forms can also be added to help customize priority templates issued
    to stakeholder or partners, or community members for opening new discussions in your
    repository..
-   **FUNDING.yml:** A FUNDING file displays a sponsor button in your repository to increase the
    visibility of funding options for your open source project.
-   **GOVERNANCE.md:** A GOVERNANCE file lets people know about how your project is governed. For
    example, it might discuss project roles and how decisions are made.
-   **Pull request & config.yml:** Issue and pull request templates help standardize the information
    you'd like contributors to include when they open issues and pull requests in your repository.
-   **SECURITY.md:** A SECURITY file gives instructions on how to report a security vulnerability in
    your project and description that hyperlinks the file. For more information, see [Adding a
    security policy to your
    repository](https://docs.github.com/en/code-security/getting-started/adding-a-security-policy-to-your-repository).
-   **SUPPORT.md:** A SUPPORT file lets people know about ways to get help with your project. For
    more information, see [Adding support resources to your
    project](https://docs.github.com/en/communities/setting-up-your-project-for-healthy-contributions/adding-support-resources-to-your-project).

----------------------------------------------------------------------------------------------------

### Practice Exercise: Mock Audit {#practice-exercise}

To test your understanding, imagine you are the auditee for this repository. An auditor asks you the
following question:

> *“Please show me the raw data source for the emission factor of 75 tC/ha. Where is that
> documented, and what is the evidence that it was used correctly?”*

**Your task:**

1.  Navigate the repository folders to locate the file(s) that would answer this question.
2.  Explain what you would present to the auditor and why it demonstrates completeness and
    integrity.
3.  Consider the risks: What would the auditor be concerned about here (e.g., Inherent , Control,
    Detection Risk)? Your response should demonstrate an understanding of why the auditor is asking
    this question.

### Appendix I: ISO Data Governance

The data management principles of traceability, integrity, and reproducibility are key to GHG
audits. These principles are supported by a range of ISO standards. This appendix provides an
overview of those ISO standards relevant to data governance and quality.

##### ISO 27001 and ISO 8000

Two widely discussed ISO standards for data governance are ISO 27001 and ISO 8000. ISO 27001 is a
standard for Information Security Management Systems (ISMS), while ISO 8000 focuses on data quality
management. Implementing the ISO 27001 standard is a deliverable of information security management
under the IT governance program and not solely a data governance deliverable. However, data
governance maturity is crucial for complying with ISO 27001 and ensuring data security,
confidentiality, and integrity in information management practices. ISO 8000 aims to ensure that
data used in various contexts, such as business processes, analytics, and decision-making, meets
certain quality standards. It covers all the key elements of data—Syntax, Provenance, Completion,
Accuracy, and Certification—providing a standard to measure and certify data quality.

##### ISO/IEC 38505–1:2017

This is a standard for data governance that provides a set of guidelines for the governance of data
within an organization. It considers data governance as a subset of IT governance, which in turn is
a subset of organizational governance, and defines clear responsibilities for the governing body and
oversight mechanisms. At its core, it provides a model for evaluating, directing, and monitoring the
handling and usage of data in an organization.

Other Relevant ISO Standards

##### ISO 22745

This standard focuses on master data exchange between organizations. It specifies data requirements
for messages containing master data, including syntax, semantic encoding, and portability. It is
often used in conjunction with ISO 8000 to realize the benefits of assessing and improving data
quality.

##### ISO 3166

This standard defines codes for country names, which can be utilized for consistent external
reference data in business applications to reduce time and effort in data integration and analytical
tasks.

##### ISO/IEC 11179

This metadata registry (MDR) standard provides a framework for representing metadata for an
organization to make data understandable and shareable. It provides guidance to software developers
building metadata repositories.

##### ISO/IEC 27701:2019

This standard serves as a data privacy extension to ISO 27001, providing a framework for
organizations to establish systems that facilitate compliance with data privacy regulations like
GDPR.

### Appendix: ISO Geodata Standards & Performance Testing

This appendix provides resources and context for trainees interested in debugging and validating
geospatial operations using the liblwgeom library.

#### ISO Data Governance and Standards

-   The `liblwgeom` library was derived according to the OpenGIS Simple Features Access (SFA)
    geometry rules (ISO 19125). This TopGeometry model is derived using open-source platform-neutral
    architecture for simple feature geometry functions. The base class includes specific subclasses
    for handling spatial data objects of Points, Curves, Surfaces, and a GeometryCollection `sfc`.
-   This part of the SFA implements a profile of the spatial schema in ISO 19107:2003. The library's
    topology support is based on the ISO SQL/MM (ISO 13249) standard. These standards help ensure
    that data used in business processes meets specific quality standards for syntax, provenance,
    completeness, and accuracy, aligning with principles of ISO 8000 (Data Quality Management).
-   The liblwgeom library uses the CUnit test framework for unit testing, which provides a
    gold-standard and lightweight method to calibrate and debug spatial operations.
-   The make check command runs a set of regression tests for the entire PostGIS system, which helps
    verify the integrity of the installation and its dependencies, such as PROJ4 (for cartographic
    projections) and GEOS (for topological tests).
    
##### Runtime Log {#runtime-log}

```{r, class.source = c("numCode", "r", "numberLines")}
devtools::session_info()
#Sys.getenv()
```
